## 뉴럴 네트워크 임베딩을 이용한 추천 시스템 만들기(Building a Recommendation System Using Neural Network Embeddings)
딥러닝과 위키피디아를 이용해서 책 추천 시스템을 어떻게 만들 수 있을까? 🤔 🤔 (How to use deep learning and Wikipedia to create a book recommendation system)
[원문 링크](https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9)
> 이 튜토리얼은

* Keras
* Recommendation System
* Neural Network
* Embedding

### Introduction

딥러닝은 몇 몇 [놀라운 일](https://blog.statsbot.co/deep-learning-achievements-4c563e034257?gi=f0d6d88ccb09)들을 할 수 있지만, 종종 그 용도는 [학술지](https://arxiv.org/abs/1301.3781)에 가려지거나 대기업에만 제공되는 [컴퓨팅 자원](https://dawn.cs.stanford.edu/benchmark/ImageNet/train.html)을 요구합니다.  그럼에도 불구하고, 고급 학위를 요구하지 않는 개인용 컴퓨터에서 할 수 있는 딥러닝 어플리케이션들이 있습니다. 이 튜토리얼에서 우리는 신경망 임베딩(neural network embeddings)을 사용하여 책에 관한 모든 위키피디아 기사를 사용하는 책 추천 시스템을 만드는 방법을 이야기할 것입니다.

우리의 추천 시스템은 유사한(similarity) 위키피디아 페이지로 연결되는 책들이 서로 비슷하다는 생각을 바탕으로 만들어질 것입니다. 우리는 이러한 유사도를 나타낼 수 있고, 따라서 신경망을 이용하여 책과 위키피디아 링크를 임베딩하는 것을 배움으로써 추천 시스템을 만들 수 있습니다. 최종 결과는 효과적인 추천 시스템이며 딥러닝의 실질적인 사용입니다. 😊

<br></br>

![cosine_distance_of_books](./media/130_1.png)
*figure1 : 스티븐 호킹의 A Brief History of Time 라는 책과 가장 유사도가 높은 책들*

<br></br>

이 프로젝트의 전체 코드는 GitHub에서 [Jupyter 노트북](https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb)으로 사용할 수 있습니다. 만약 GPU가 없다면 GPU로 신경망을 무료로 학습시킬 수 있는 [Kaggle의 노트북](https://www.kaggle.com/willkoehrsen/neural-network-embedding-recommendation-system)도 있습니다. 이 튜토리얼은 [이전 튜토리얼](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526)에서 다루었던 신경망 개념과 함께 적용하는 것에 초점을 맞출 것이다. (우리가 사용할 데이터를 검색하는 방법은 – [all book articles on Wikipedia](https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c) - 이 튜토리얼을 보세요.)

이 프로젝트는 [딥러닝 쿡북](http://shop.oreilly.com/product/0636920097471.do)에서 채용했는데, 딥러닝의 실제 예를 보여주는 훌륭한 [책](https://github.com/DOsinga/deep_learning_cookbook)이다. 👍

<br></br>
<br></br>

### 신경망 임베딩 (Neural Network Embeddings)

임베딩은 이산형 변수를 연속 벡터로 나타내는 방법 중 하나입니다. 원-핫 인코딩과 같은 인코딩 방법과 대조적으로 신경망 임베딩은 저차원이며 학습을 해야합니다. 즉, 임베딩 공간에 서로 유사한 실체를 서로 가깝게 배치한다는 뜻입니다.

임베딩을 하기위해서, 우리는 신경망 모델과 지도학습(supervised learning) 작업이 필요합니다. 우리 네트워크의 최종 결과는 각각의 책들을 50개의 연속된 숫자 벡터로 표현하는 것이 될 것입니다.

임베딩을 학습하는 과정은 그다지 재미있지는 않습니다. 😑--그저 벡터들일 뿐이에요.--임베딩이 된 후 세 가지 주요 목적으로 사용할 수 있습니다.

1. 임베딩 공간에서 가장 가까운 이웃(nearest neighbor) 찾기
2. 기계 학습 모델에 대한 입력주기
3. 저차원 시각화하기

> 위에서 **가까운 이웃** 은 번역자가 좋아하는 spiderman 의 별명 friendly neighborhood spider man 을 <U>친절한 이웃 스파이더맨</U> 과 같이 예쁘게 번역한 단어같지만 `nearest neighbor` 라는 알고리즘 이름입니다.

이 프로젝트에서는 첫 번째 목적을 주로 다루지만, 임베딩에서 시각화를 하는 방법도 살펴볼 것입니다. 신경망 임베딩의 실질적인 적용에는 기계 번역을 위한 [<U>워드 임베딩(word embedding)</U>](https://towardsdatascience.com/building-a-recommendation-system-using-neural-network-embeddings-1ef92e5c80c9) 및 카테고리컬 변수를 위한 [<U>엔티티 임베딩(entity embedding)</U>](https://arxiv.org/abs/1604.06737) 입니다.

<br></br>
<br></br>

### 데이터 : 위키피디아의 모든 책들 (Data: All Books on Wikipedia)

Data science 프로젝트와 마찬가지로 고품질의 데이터셋부터 시작해야 합니다. 이 [글](https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c)에서, 우리는 위키피디아에 있는 모든 기사들을 다운로드 받고 처리하는 방법을 보았습니다. 책에 관한 모든 페이지를 검색하면서 말이죠. 책 제목, 기본 정보, 다른 위키피디아 페이지(위클링크)를 가리키는 링크, 외부 사이트 링크를 저장했습니다. 추천 시스템을 만들기 위해 필요한 정보는 *제목* 과 *위키링크* 뿐입니다.

<br></br>

```
Book Title: 'The Better Angels of Our Nature'
Wikilinks:  
['Steven Pinker',
  'Nation state',
  'commerce',
  'literacy',
  'Influence of mass media',
  'Rationality',
  "Abraham Lincoln's first inaugural address",
  'nature versus nurture',
  'Leviathan']
```

<br></br>

신경망을 사용할 때도 데이터를 탐색하고 정리하는 것이 중요하며 노트북에서 원시 데이터를 몇 가지 수정했습니다. 예를 들어 가장 많이 연결된 페이지를 보면 다음과 같습니다.

<br></br>

![가장_많이_연결된_페이지](./media/130_2.png)
*figure2 : 위키피디아의 페이지는 위키피디아에 있는 책들로 가장 자주 연결됩니다.*

<br></br>

우리는 figure2 를 보고 상위 4개 페이지는 일반적이며 추천 시스템을 만드는 데 도움이 되지 않는다는 것을 알 수 있습니다. 책의 형식은 그 내용과 관련이 없다는 것을 알고 있기 때문입니다.🤨 한 책이 `종이로 된 책` 인지 `하드커버` 인지 아는 것은 우리나 신경망을 통해 그와 내용이 유사한 다른 책을 알아낼 수 없습니다. 그러므로, 우리는 책을 구별하기 위해 이 연결고리를 제거할 것입니다.

최종 목적에 대해 생각하는 것은 데이터 정리 단계에서 도움이 될 수 있고, 이러한 방법만으로도 추천 시스템을 크게 개선할 수 있다.

순수한 호기심에서, 저든 위키피디아에 관한 다른 책들과 가장 관련이 많은 책들을 찾고 싶었습니다. figure 3 은 "가장 많이 연결된" 10권의 위키피디아 책입니다.

<br></br>

![](./media/130_3.png)
*figure3 : 위키피디아에 관한 책은 위키피디아에 관한 다른 책들과 가장 자주 연결되어 있습니다.*

<br></br>

이것은 참고서적과 고전적 서적들 입니다.

데이터 클리닝 후, 우리는 **41758** 개의 유니크한 위키링크와 **37020** 개의 유니크한 책을 가지게 되었습니다. 모든 사람들을 위한 책이 그곳에 있기를 바랍니다! 🙏

일단 데이터가 깨끗하다고 확신하게 되면, 우리는 라벨로 표시된 데이터와 함께 지도학습(supervised learning) 을 해야합니다.

<br></br>
<br></br>

### 지도학습하기 (Supervised Learning Task)

**의미 있는 임베딩** 을 학습하기 위해서는, 우리의 신경 네트워크가 목표를 달성하도록 훈련되어야 합니다. 프로젝트의 가정(비슷한 책이 비슷한 위키피디아 페이지와 연결된다는)을 통해 우리는 문제를 다음과 같이 정의할 수 있습니다. (책 제목, 위키링크) 쌍이 주어지면, 위키링크(Wikilink)가 있는지 확인합니다.

신경망에 책 기사를 제공할 필요는 없습니다. 대신 책 제목, 위키링크, 라벨로 구성된 수십만 개의 교육 예제를 제공할 것입니다. 신경망에 True 데이터셋과 False 데이터셋을 입력하고, 마지막으로 임베딩에 대한 정보를 학습하여 wikilink가 책 페이지에 표시되는 시점을 구분합니다.

지도학습(supervised learning)을 표현하는 것이 이 프로젝트의 가장 중요한 부분입니다. 임베딩은 특정 작업에 대해 학습되며 해당 문제와만 관련이 있습니다. 만약 우리가 어떤 책이 제인 오스틴이 썼는지 결정하는 것이 우리의 임무였다면, 이 임베딩들은 그 목표를 반영하여 오스틴이 쓴 책들을 임베딩 공간에서 더 가깝게 함께 놓았을 것이다. 우리는 책 페이지에 특정 위키링크(Wikilink)가 있는지 알아보기 위한 학습을 통해, 네트워크는 콘텐츠 측면에서 유사한 책을 서로 가까이 두는 것을 배우기를 희망하고 있습니다.

일단 우리가 해야될 일을 요약하자면, 우리는 그것을 코드로 구현해야 합니다. 신경망에는 정수 입력만 할 수 있기 때문에 각 고유 책자에서 정수로의 매핑을 만듭니다.

<br></br>

```python
# 책 이름과 책의 고유 인덱스 index 맵핑
book_index = {book[0]: idx for idx, book in enumerate(books)}

book_index['Anna Karenina']
22494
```

<br></br>

우리는 링크를 통해서도 같은 일을 합니다. 그런 다음 학습 데이터 셋을 만들기 위해 데이터에 있는 모든(책, Wikilink) 쌍을 나열합니다. 이를 위해서는 각 책을 반복하고 페이지에 있는 각 위키링크에 대한 예를 기록해야 합니다.

<br></br>

```python
pairs = []

# 각각 책이 나오도록 반복
for book in books:

    title = book[0]
    book_links = book[2]
    # 책에 관한 글에 있는 wikilinks 들을 반복
    for link in book_links:
        # 책의 인덱스와 링크 페어 저장
        pairs.extend((book_index[title],                
                      link_index[link]))
```

<br></br>

이것은 우리가 모델을 훈련시키기 위해 표본으로 추출할 수 있는 총 **772798개** 의 True 인 예를 보여줍니다. False 예제를 생성하려면(나중에 수행됨) 링크 색인과 책 인덱스를 무작위로 선택하고 쌍에 없는지 확인한 다음 부정의 의미로 사용하면 됩니다.

##### 학습/테스트 세트에 대한 참고 사항 (Note about Training / Testing Sets)

별도의 검증(validation set) 및 테스트 셋을 사용하는 것이 일반적인 지도학습(supervised learning) 작업의 필수 사항이지만, 이 경우 가장 정확한 모델을 만드는 것이 아니라 내장 모델을 생성하는 것이 주된 목표입니다. 예측 작업은 임베딩을 위해 네트워크를 교육하는 수단일 뿐입니다. 교육이 끝나면 새 데이터에 대한 모델을 테스트하지 않으므로 성능을 평가하거나 검증 세트를 사용하여 오버핏(overfitting)을 방지할 필요가 없습니다. 최상의 임베딩 값을 확보하기 위해 모든 데이터를 학습에 사용할 것입니다. 😬

<br></br>
<br></br>

### 임베딩 모델 (Embedding Model)

신경망이 기술적으로 복잡한 것처럼 들리지만, [케라스 딥러닝 프레임워크](https://keras.io)로 비교적 쉽게 구현할 수 있습니다. 👍👍 텐서플로우(TensorFlow)는 더 많은 제어력을 제공할 수 있지만, 케라스(Keras)를 능가할 수는 없습니다.)

임베딩 모델은 5개의 레이어를 가지고 있습니다.

1. Input: 책 및 링크에 대한 병렬 입력.
2. Embedding: 책 및 링크를 위한 병렬 길이 50개의 임베딩.
3. Dot: 내적(dot product)을 계산하여 임베딩 합치기.
4. Reshape: 임베딩 shape 를 단일 숫자로 형성.
5. Dense: 시그모이드 활성화를 이용한 출력.

[embedding neural network](https://keras.io/layers/embeddings/)에서 임베딩은 목표의 손실을 최소화하기 위해 훈련 중에 조정되는 신경 네트워크의 매개변수(weight)입니다. 신경망은 책과 링크를 정수로 사용하여 0과 1 사이의 예측을 출력하며, 이는 실제 값과 비교됩니다. 이 모델은 [`Adam Optimizer`(Stochastic Gradient Descent Descent Descent의 변형)](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)로 컴파일되며, 이 과정에서 이 이진 분류 문제에 대한 `binary_crossentropy` 값을 최소화합니다.

<br></br>

아래는 모델에 대한 코드입니다:

```python
from keras.layers import Input, Embedding, Dot, Reshape, Dense
from keras.models import Model

def book_embedding_model(embedding_size = 50, classification = False):
    """Model to embed books and wikilinks using the Keras functional API.
       Trained to discern if a link is present in on a book's page"""

    # 1차원의 입력
    book = Input(name = 'book', shape = [1])
    link = Input(name = 'link', shape = [1])

    # 책 임베딩 (shape will be (None, 1, 50))
    book_embedding = Embedding(name = 'book_embedding',
                               input_dim = len(book_index),
                               output_dim = embedding_size)(book)

    # 링크 임베딩 (shape will be (None, 1, 50))
    link_embedding = Embedding(name = 'link_embedding',
                               input_dim = len(link_index),
                               output_dim = embedding_size)(link)

    # 내적으로 책 임베딩과 링크 임베딩을 한 개의 임베딩 벡터로 변형
    # (shape will be (None, 1, 1))
    merged = Dot(name = 'dot_product', normalize = True,
                 axes = 2)([book_embedding, link_embedding])

    # 단일 숫자로 shape 변형 (shape will be (None, 1))
    merged = Reshape(target_shape = [1])(merged)

    # 분류를 위한 결과값 출력
    out = Dense(1, activation = 'sigmoid')(merged)
    model = Model(inputs = [book, link], outputs = out)

    # 원하는 optimizer 와 loss 함수로 모델 학습 시작
    model.compile(optimizer = 'Adam', loss = 'binary_crossentropy',
                  metrics = ['accuracy'])

    return model
```

<br></br>

이 프레임워크는 많은 내장 모델에 사용할 수 있습니다. 중요한 점은 임베딩이 모델 매개변수(weight)이며 또한 우리가 원하는 최종 결과라는 것입니다. 우리는 그 모델이 정확한지 그다지 신경쓰지 않을 것 입니다, 우리가 원하는 것은 적절한 임베딩 벡터입니다.

우리는 모델의 가중치에 익숙해져 정확한 예측을 할 수 있습니다. 하지만 내재된 모델에서 가중치가 목표이고 예측은 내재된 것을 학습하 수단입니다.

<br></br>

약 400 만 개의 weight 를 model summary 로 확인할 수 있습니다:

```python


__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
book (InputLayer)               (None, 1)            0                                            
__________________________________________________________________________________________________
link (InputLayer)               (None, 1)            0                                            
__________________________________________________________________________________________________
book_embedding (Embedding)      (None, 1, 50)        1851000     book[0][0]                       
__________________________________________________________________________________________________
link_embedding (Embedding)      (None, 1, 50)        2087900     link[0][0]                       
__________________________________________________________________________________________________
dot_product (Dot)               (None, 1, 1)         0           book_embedding[0][0]             
                                                                 link_embedding[0][0]             
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 1)            0           dot_product[0][0]                
==================================================================================================
Total params: 3,938,900
Trainable params: 3,938,900
Non-trainable params: 0
```

<br></br>

이 접근법으로 우리는 책뿐만 아니라 책으로 연결된 모든 위키피디아의 페이지를 비교할 수 있는 링크도 얻을 수 있습니다.

<br></br>
<br></br>

### 학습 샘플 만들기 (Generating Training Samples)

신경망은 [<U>batch learner</U>](https://en.wikipedia.org/wiki/Online_machine_learning#Batch_learning)입니다. 왜냐하면 한 번에 하나의 작은 표본 집합(관찰)을 통해 epoch 라고 불리는 많은 라운드에서 훈련을 받기 때문입니다. 신경 네트워크를 훈련시키는 일반적인 방법은 발전기를 이용하는 것이다. 전체 결과가 메모리에 저장되지 않도록 검체 배치를 산출(반환하지 않음)하는 기능입니다. 이 문제의 문제는 아니지만 제너레이터의 이점은 대규모 교육 세트를 모두 메모리에 로드할 필요가 없다는 것입니다.
