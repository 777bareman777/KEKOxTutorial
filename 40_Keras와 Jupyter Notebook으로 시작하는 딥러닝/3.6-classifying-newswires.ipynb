{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/homme/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/homme/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph 1 : Original\n",
    "# Classifying newswires: a multi-class classification example\n",
    "\n",
    "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
    "\n",
    "----\n",
    "\n",
    "In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network. \n",
    "But what happens when you have more than two classes? \n",
    "\n",
    "In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many \n",
    "classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one \n",
    "category, the problem is more specifically an instance of \"single-label, multi-class classification\". If each data point could have \n",
    "belonged to multiple categories (in our case, topics) then we would be facing a \"multi-label, multi-class classification\" problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph 1 : Original\n",
    "# 신문기사 분류해보기 : 다중 클래스 분류 예제\n",
    "\n",
    "이 Notebook은 Deep Learning with Python의 3장 5절에 나오는 예제 코드가 포함되어 있습니다.  \n",
    "원본 텍스트(Deep Learning with Python)에 훨씬 더 많은 자료, 특히 추가 설명과 그림들이 포함되어 있습니다.  \n",
    "여기에서는 예제 코드와 코드에 관련된 설명만 제공됩니다.\n",
    "\n",
    "----\n",
    "\n",
    "### Paragraph 1 : Tranlation\n",
    "앞절에는 우리는 densely-connected neural network를 이용해서 상호 베타적인 두 개의 클래스를 분류하는 방법을 배웠습니다. 그렇다면 분류해야 하는 클래스가 두 가지 이상이라면 어떻게 해야할까요? 이번 장에서는 뉴스 레터들(Reuters newswires)을 46개의 상호-베타적인 주제로 분류해 보도록 하겠습니다. 이를 \"다중-클래스 분류\" 문제라고 합니다. \"다중-클래스\" 라고 부르는 이유는 분류해야 할 클래스들이 많기 때문입니다. 그리고 더불어 각 데이터 포인트가 오로지 한 카테고리 안에 속하는 경우에 \"단일-레이블, 다중-객체 분류\" 문제라고 합니다. 만약 각 데이터 포인트가 여러 카테고리에 속한다면, 이를 \"다중-레이블, 다중-클래스 분류\" 문제라고 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph 2 : Original\n",
    "\n",
    "## The Reuters dataset\n",
    "\n",
    "\n",
    "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
    "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
    "topic has at least 10 examples in the training set.\n",
    "\n",
    "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph 2 : Tranlation\n",
    "\n",
    "## 로이터(Reuters) 데이터셋\n",
    "\n",
    "이번에는 _로이터 데이터셋_ 을 가지고 실습을 진행해 보겠습니다. 이 데이터셋은 1986년 로이터에서 발간된 뉴스 단신들로 구성되어 있습니다. 이번에 할 문제도 아주 쉽습니다. 텍스트 분류를 위한 토이 데이터셋으로 아주 널리 사용되는 데이터셋입니다. 총 46가지 주제가 있습니다. 일부 주제들은 학습 샘플이 비교적 많으 반면 어떤 주제는 샘플이 10개 조차 되지 않습니다. IMDB와 MNIST처럼 _로이터 데이터셋_ 또한 Keras 에서 불러올 수 있습니다. 지금 당장 살펴봅시다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 7s 3us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the data.\n",
    "\n",
    "We have 8,982 training examples and 2,246 test examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB 데이터셋에서와 마찬가지고 인자 `num_words = 10000`을 통해서 데이터 내에서 가장 빈번하게 발생하는 단어들 10,000개 만으로 데이터를 제한하도록 하겠습니다. \n",
    "\n",
    "_로이터 데이터셋_ 에는 8,982 개의 학습 샘플과 2,246개의 테스트 샘플이 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8982"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2246"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the IMDB reviews, each example is a list of integers (word indices):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB 에서 봤던 것 처럼 각 샘플들은 integers 리스트로 구성됩니다. (단어에 인덱스를 의미합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 245,\n",
       " 273,\n",
       " 207,\n",
       " 156,\n",
       " 53,\n",
       " 74,\n",
       " 160,\n",
       " 26,\n",
       " 14,\n",
       " 46,\n",
       " 296,\n",
       " 26,\n",
       " 39,\n",
       " 74,\n",
       " 2979,\n",
       " 3554,\n",
       " 14,\n",
       " 46,\n",
       " 4689,\n",
       " 4329,\n",
       " 86,\n",
       " 61,\n",
       " 3499,\n",
       " 4795,\n",
       " 14,\n",
       " 61,\n",
       " 451,\n",
       " 4329,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can decode it back to words, in case you are curious:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서는 여러분이 원본 문장에 궁금할 때, Integers를 다시 문장으로 디코딩 하는 방법을 소개합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 2s 4us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label associated with an example is an integer between 0 and 45: a topic index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 샘플의 해당 레이블은 0 - 45 사이의 Integer로 표시됩니다: 토픽에 대한 인덱스입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 3\n",
    "## Preparing the data\n",
    "\n",
    "We can vectorize the data with the exact same code as in our previous example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비하기\n",
    "\n",
    "이전 예제에서 사용했던 동일한 코드를 이용해서 데이터를 벡터화 시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" \n",
    "encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". \n",
    "For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1. \n",
    "In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블을 벡터화 하기 위한 방법에는 두 가지 옵션이 존재합니다: 하나는 레이블 리스트를 단순히 Integer tensor로 캐스팅하는 방법입니다. 혹은, \"ont-hot\" 인코딩을 사용할 수도 있습니다. 원-핫 인코딩은 범주형 데이터 포멧이 아주 널리 사용되는 방법입니다. \"범주형 인코딩(categorical encoding\" 이라고도 합니다. 원-핫 인코딩에 대한 조금 더 상세한 설명은 6장 1절을 참고하시기 바랍니다. 우리 가진 레이블들을 가지고 원-핫 인코딩을 하는 경우에는 레이블 인덱스가 위치한 자리만 1로 표시하고 나머지 자리는 모두 0으로 채워넣습니다.  \n",
    "가령 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# Our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 MNIST 예제에서도 보셨겠지만, Keras에는 이를 실행하기 위한 빌트인(built-in) 이 이미 존재합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 4\n",
    "## Building our network\n",
    "\n",
    "\n",
    "This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to \n",
    "classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the \n",
    "dimensionality of the output space is much larger. \n",
    "\n",
    "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. \n",
    "If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each \n",
    "layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a \n",
    "16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, \n",
    "permanently dropping relevant information.\n",
    "\n",
    "For this reason we will use larger layers. Let's go with 64 units:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 4\n",
    "## 네트워크 구성하기\n",
    "\n",
    "뉴스 주제를 분류하는 문제는 앞서 영화 리뷰를 분류하는 문제와 아주 유사합니다. 두 경우 모두 짧은 텍스트들을 분류하는 문제입니다. \n",
    "다만 출력 클래스가 2개에서 46개로 바뀌었습니다. 즉, 출력 공간의 차원이 훨씬 더 큽니다.\n",
    "\n",
    "우리가 이전에 사용하기도 했었던 `Dense` 레이어에서는, 각 레이어는 오직 이전 레이어의 출력의 정보에만 접근할 수 있습니다. \n",
    "만일 어떤 레이어가 분류 문제에 필요한 정보를 잃는다면, 이 정보는 이 후의 레이어에서 복구할 수 없습니다. 어떤 레이어가 \"정보 병목\"이 될 수 있는 것입니다. \n",
    "앞선 예제에서, 우리는 16차원의 중간 레이어들을 사용했습니다. 하지만 16-차원은 46개의 다양한 클래스를 나누기에는 턱없이 부족합니다. 이러한 크기가 작은 레이어들은 정보 병목으로서의 역할을 하여 정보를 영구적으로 떨어뜨립니다.\n",
    "\n",
    "이러한 이유로 인해 우리는 이제 조금 더 큰 레이어를 사용할 것입니다. 우선 64개의 유닛으로 시작해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two other things you should note about this architecture:\n",
    "\n",
    "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
    "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
    "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
    "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
    "\n",
    "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
    "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
    "distance between these two distributions, we train our network to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 설계한 아키텍쳐에 대해서 여러분이 알고 넘어가야 할 것이 두 가지 있습니다. \n",
    "\n",
    "* 이 네트워크의 마지막은 46 사이즈의 `Dense` 레이어로 구성됩니다. 이는 네트워크의 출력이 46-차원 벡터임을 의미합니다. 이 벡터의 각 요소는 하나의 출력 클래스입니다.  \n",
    "\n",
    "* 마지막 레이어에서는 활성 함수로 `softmax`를 이용합니다. 우리는 이미 MNIST 예제에서 살펴본 적이 있습니다. `softmax`라는 것은 출력이 46 개의 출력 클래스에 대한 `확률 분포` 임을 의미합니다. 즉, 모든 입력 샘플에서 네트워크가 46-차원 벡터를 출력하는데, `output[i]`는 입력 샘플이 `i`번째 클래스에 속할 확률을 의미합니다. 46개의 스코어를 모두 더하면 1이 됩니다.\n",
    "\n",
    "이 경우 가장 쓸만한 손실 함수는 `categorical_crossentropy` 입니다. 이는 두 개의 확률분포 사이의 거리를 측정하는 방법입니다. 이 예시의 경우에는 네트워크의 출력의 확률분포와 정답 레이블의 정답분포 사이의 거리입니다. 이 두 분포 간의 거리를 최소화 함으로써, 우리는 출력이 정답 레이블에 가까워 지도록 학습을 시킬 수 있는 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 5\n",
    "## Validating our approach\n",
    "\n",
    "Let's set apart 1,000 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 5\n",
    "## 모델 검증하기\n",
    "\n",
    "앞서 1,000개의 **학습 데이터**(training data) 를 **검증 집합**(validation set) 으로 남겨두었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크를 20 에포크 동안 학습시켜봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 241us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7208 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 1.4452 - acc: 0.6879 - val_loss: 1.3459 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 1.0953 - acc: 0.7651 - val_loss: 1.1708 - val_acc: 0.7430\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 100us/step - loss: 0.8697 - acc: 0.8165 - val_loss: 1.0793 - val_acc: 0.7590\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.7034 - acc: 0.8472 - val_loss: 0.9844 - val_acc: 0.7810\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.5667 - acc: 0.8802 - val_loss: 0.9411 - val_acc: 0.8040\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.4581 - acc: 0.9048 - val_loss: 0.9083 - val_acc: 0.8020\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 102us/step - loss: 0.3695 - acc: 0.9231 - val_loss: 0.9363 - val_acc: 0.7890\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 102us/step - loss: 0.3032 - acc: 0.9315 - val_loss: 0.8917 - val_acc: 0.8090\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.2537 - acc: 0.9414 - val_loss: 0.9071 - val_acc: 0.8110\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 99us/step - loss: 0.2187 - acc: 0.9471 - val_loss: 0.9177 - val_acc: 0.8130\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 100us/step - loss: 0.1873 - acc: 0.9508 - val_loss: 0.9027 - val_acc: 0.8130\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.1703 - acc: 0.9521 - val_loss: 0.9323 - val_acc: 0.8110\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.1536 - acc: 0.9554 - val_loss: 0.9689 - val_acc: 0.8050\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 100us/step - loss: 0.1390 - acc: 0.9560 - val_loss: 0.9686 - val_acc: 0.8150\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 100us/step - loss: 0.1313 - acc: 0.9560 - val_loss: 1.0220 - val_acc: 0.8060\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.1217 - acc: 0.9579 - val_loss: 1.0254 - val_acc: 0.7970\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 1s 102us/step - loss: 0.1198 - acc: 0.9582 - val_loss: 1.0430 - val_acc: 0.8060\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.1138 - acc: 0.9597 - val_loss: 1.0955 - val_acc: 0.7970\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 0.1111 - acc: 0.9593 - val_loss: 1.0674 - val_acc: 0.8020\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display its loss and accuracy curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실과 정확도 커브를 출력시켜봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYFOW59/HvDYwggiCLIiAMuAKCLBNAEQG3F/egxKi4\nRkWJRo3HvHLQqEclUY9bUF4NJhoNo8SjxzUoMZEEdwWCICIBFRRFNmUTjAzc7x9PdU8zzNLDTE31\nzPw+11VXV1dVV99d01N3P0s9Ze6OiIgIQIOkAxARkdyhpCAiImlKCiIikqakICIiaUoKIiKSpqQg\nIiJpSgpSrcysoZltNLNO1bltksxsPzOr9r7bZna0mS3JeL7QzAZns+1OvNfvzGzczr6+nP3eamZ/\nqO79SnIaJR2AJMvMNmY8bQr8G9gaPb/E3Qsrsz933wo0q+5t6wN3P7A69mNmFwFnu/vQjH1fVB37\nlrpPSaGec/f0STn6JXqRu/+1rO3NrJG7F9VEbCJS81R9JOWKqgf+ZGZPmNkG4GwzO9TM3jaztWa2\n3MwmmFletH0jM3Mzy4+eT47Wv2RmG8zsLTPrUtlto/XHmdm/zGydmd1nZm+Y2fllxJ1NjJeY2WIz\n+8bMJmS8tqGZ3WNma8zsE2B4OcfnOjObUmLZRDO7O5q/yMwWRJ/n4+hXfFn7WmZmQ6P5pmb2xyi2\n+UC/Etteb2afRPudb2YnR8t7AvcDg6OqudUZx/amjNdfGn32NWb2rJntnc2xqYiZjYjiWWtmr5rZ\ngRnrxpnZl2a23sw+yvisA81sdrR8hZn9d7bvJzFwd02acHeAJcDRJZbdCnwPnET4EbEr8ANgAKGk\n2RX4F3B5tH0jwIH86PlkYDVQAOQBfwIm78S2ewIbgFOidVcDW4Dzy/gs2cT4HNACyAe+Tn124HJg\nPtARaA3MCP8qpb5PV2AjsFvGvlcCBdHzk6JtDDgS2Az0itYdDSzJ2NcyYGg0fyfwd2APoDPwYYlt\nTwf2jv4mZ0Ux7BWtuwj4e4k4JwM3RfPHRjH2BpoA/w94NZtjU8rnvxX4QzTfLYrjyOhvNA5YGM33\nAJYC7aJtuwBdo/n3gDOj+ebAgKT/F+rzpJKCZON1d3/B3be5+2Z3f8/d33H3Inf/BJgEDCnn9U+5\n+0x33wIUEk5Gld32RGCOuz8XrbuHkEBKlWWMv3b3de6+hHACTr3X6cA97r7M3dcAt5XzPp8AHxCS\nFcAxwDfuPjNa/4K7f+LBq8DfgFIbk0s4HbjV3b9x96WEX/+Z7/ukuy+P/iaPExJ6QRb7BRgF/M7d\n57j7d8BYYIiZdczYpqxjU54zgOfd/dXob3QbIbEMAIoICahHVAX5aXTsICT3/c2stbtvcPd3svwc\nEgMlBcnG55lPzOwgM/uzmX1lZuuBm4E25bz+q4z5TZTfuFzWtu0z43B3J/yyLlWWMWb1XoRfuOV5\nHDgzmj8rep6K40Qze8fMvjaztYRf6eUdq5S9y4vBzM43s/ejapq1wEFZ7hfC50vvz93XA98AHTK2\nqczfrKz9biP8jTq4+0LgPwh/h5VRdWS7aNMLgO7AQjN718yOz/JzSAyUFCQbJbtj/pbw63g/d98d\nuIFQPRKn5YTqHADMzNj+JFZSVWJcDuyT8byiLrNPAkebWQdCieHxKMZdgaeAXxOqdloCf8kyjq/K\nisHMugIPAGOA1tF+P8rYb0XdZ78kVEml9tecUE31RRZxVWa/DQh/sy8A3H2yuw8iVB01JBwX3H2h\nu59BqCK8C3jazJpUMRbZSUoKsjOaA+uAb82sG3BJDbzni0BfMzvJzBoBVwJtY4rxSeAqM+tgZq2B\na8vb2N2/Al4H/gAsdPdF0arGwC7AKmCrmZ0IHFWJGMaZWUsL13FcnrGuGeHEv4qQHy8mlBRSVgAd\nUw3rpXgCuNDMeplZY8LJ+TV3L7PkVYmYTzazodF7/4LQDvSOmXUzs2HR+22Opm2ED3COmbWJShbr\nos+2rYqxyE5SUpCd8R/AeYR/+N8SGoRj5e4rgB8DdwNrgH2BfxKuq6juGB8g1P3PIzSCPpXFax4n\nNBynq47cfS3wc+AZQmPtSEJyy8aNhBLLEuAl4LGM/c4F7gPejbY5EMish38FWASsMLPMaqDU618m\nVOM8E72+E6GdoUrcfT7hmD9ASFjDgZOj9oXGwB2EdqCvCCWT66KXHg8ssNC77U7gx+7+fVXjkZ1j\noWpWpHYxs4aE6oqR7v5a0vGI1BUqKUitYWbDo+qUxsAvCb1W3k04LJE6RUlBapPDgU8IVRP/Bxjh\n7mVVH4nITlD1kYiIpKmkICIiabVuQLw2bdp4fn5+0mGIiNQqs2bNWu3u5XXjBmphUsjPz2fmzJlJ\nhyEiUquYWUVX5gOqPhIRkQxKCiIikqakICIiabWuTUFEataWLVtYtmwZ3333XdKhSBaaNGlCx44d\nycsra+ir8ikpiEi5li1bRvPmzcnPzycMTiu5yt1Zs2YNy5Yto0uXLhW/oBT1ovqosBDy86FBg/BY\nWKlb0YvUb9999x2tW7dWQqgFzIzWrVtXqVRX50sKhYUwejRs2hSeL10angOMqvK4kCL1gxJC7VHV\nv1WdLylcd11xQkjZtCksFxGR7cWWFMxsHzObbmYfmtl8M7uylG2Gmtk6M5sTTTdUdxyffVa55SKS\nW9asWUPv3r3p3bs37dq1o0OHDunn33+f3W0XLrjgAhYuXFjuNhMnTqSwmuqWDz/8cObMmVMt+6pp\ncVYfFQH/4e6zo9v9zTKzV9z9wxLbvebuJ8YVRKdOocqotOUiUv0KC0NJ/LPPwv/Z+PFVq6pt3bp1\n+gR700030axZM6655prttnF33J0GDUr/nfvII49U+D6XXXbZzgdZh8RWUnD35e4+O5rfACyg/Hvq\nxmL8eGjadPtlTZuG5SJSvVJteEuXgntxG14cnTsWL15M9+7dGTVqFD169GD58uWMHj2agoICevTo\nwc0335zeNvXLvaioiJYtWzJ27FgOOeQQDj30UFauXAnA9ddfz7333pvefuzYsfTv358DDzyQN998\nE4Bvv/2W0047je7duzNy5EgKCgoqLBFMnjyZnj17cvDBBzNu3DgAioqKOOecc9LLJ0yYAMA999xD\n9+7d6dWrF2effXa1H7Ns1EhDs5nlA33Y/paBKYeZ2VzCzb2viW7pV/L1o4HRAJ0q+RM/9QulOn+5\niEjpymvDi+N/7qOPPuKxxx6joKAAgNtuu41WrVpRVFTEsGHDGDlyJN27d9/uNevWrWPIkCHcdttt\nXH311Tz88MOMHTt2h327O++++y7PP/88N998My+//DL33Xcf7dq14+mnn+b999+nb9++5ca3bNky\nrr/+embOnEmLFi04+uijefHFF2nbti2rV69m3rx5AKxduxaAO+64g6VLl7LLLrukl9W02BuazawZ\n8DRwlbuvL7F6NtDJ3XsR7jn7bGn7cPdJ7l7g7gVt21Y4yN8ORo2CJUtg27bwqIQgEo+absPbd999\n0wkB4IknnqBv37707duXBQsW8OGHJWurYdddd+W4444DoF+/fixZsqTUfZ966qk7bPP6669zxhln\nAHDIIYfQo0ePcuN75513OPLII2nTpg15eXmcddZZzJgxg/3224+FCxdyxRVXMG3aNFq0aAFAjx49\nOPvssyksLNzpi8+qKtakYGZ5hIRQ6O7/W3K9u693943R/FQgz8zaxBmTiMSnrIJ8XG14u+22W3p+\n0aJF/OY3v+HVV19l7ty5DB8+vNT++rvsskt6vmHDhhQVFZW678aNG1e4zc5q3bo1c+fOZfDgwUyc\nOJFLLrkEgGnTpnHppZfy3nvv0b9/f7Zu3Vqt75uNOHsfGfB7YIG7313GNu2i7TCz/lE8a+KKSUTi\nlWQb3vr162nevDm77747y5cvZ9q0adX+HoMGDeLJJ58EYN68eaWWRDINGDCA6dOns2bNGoqKipgy\nZQpDhgxh1apVuDs/+tGPuPnmm5k9ezZbt25l2bJlHHnkkdxxxx2sXr2aTSXr4mpAnG0Kg4BzgHlm\nlmqJGQd0AnD3B4GRwBgzKwI2A2e47g8qUmsl2YbXt29funfvzkEHHUTnzp0ZNGhQtb/Hz372M849\n91y6d++enlJVP6Xp2LEjt9xyC0OHDsXdOemkkzjhhBOYPXs2F154Ie6OmXH77bdTVFTEWWedxYYN\nG9i2bRvXXHMNzZs3r/bPUJFad4/mgoIC1012RGrOggUL6NatW9Jh5ISioiKKiopo0qQJixYt4thj\nj2XRokU0apRbg0OU9jczs1nuXlDGS9Jy65OIiOSwjRs3ctRRR1FUVIS789vf/jbnEkJV1a1PIyIS\no5YtWzJr1qykw4hVnR/7SEREsqekICIiaUoKIiKSpqQgIiJpSgoiktOGDRu2w4Vo9957L2PGjCn3\ndc2aNQPgyy+/ZOTIkaVuM3ToUCrq4n7vvfdudxHZ8ccfXy3jEt10003ceeedVd5PdVNSEJGcduaZ\nZzJlypTtlk2ZMoUzzzwzq9e3b9+ep556aqffv2RSmDp1Ki1bttzp/eU6JQURyWkjR47kz3/+c/qG\nOkuWLOHLL79k8ODB6esG+vbtS8+ePXnuued2eP2SJUs4+OCDAdi8eTNnnHEG3bp1Y8SIEWzevDm9\n3ZgxY9LDbt94440ATJgwgS+//JJhw4YxbNgwAPLz81m9ejUAd999NwcffDAHH3xwetjtJUuW0K1b\nNy6++GJ69OjBscceu937lGbOnDkMHDiQXr16MWLECL755pv0+6eG0k4NxPePf/wjfZOhPn36sGHD\nhp0+tqXRdQoikrWrroLqvqFY794QnU9L1apVK/r3789LL73EKaecwpQpUzj99NMxM5o0acIzzzzD\n7rvvzurVqxk4cCAnn3xymfcpfuCBB2jatCkLFixg7ty52w19PX78eFq1asXWrVs56qijmDt3Lldc\ncQV3330306dPp02b7cfqnDVrFo888gjvvPMO7s6AAQMYMmQIe+yxB4sWLeKJJ57goYce4vTTT+fp\np58u9/4I5557Lvfddx9Dhgzhhhtu4L/+67+49957ue222/j0009p3LhxusrqzjvvZOLEiQwaNIiN\nGzfSpEmTShztiqmkICI5L7MKKbPqyN0ZN24cvXr14uijj+aLL75gxYoVZe5nxowZ6ZNzr1696NWr\nV3rdk08+Sd++fenTpw/z58+vcLC7119/nREjRrDbbrvRrFkzTj31VF577TUAunTpQu/evYHyh+eG\ncH+HtWvXMmTIEADOO+88ZsyYkY5x1KhRTJ48OX3l9KBBg7j66quZMGECa9eurfYrqlVSEJGslfeL\nPk6nnHIKP//5z5k9ezabNm2iX79+ABQWFrJq1SpmzZpFXl4e+fn5pQ6XXZFPP/2UO++8k/fee489\n9tiD888/f6f2k5IadhvC0NsVVR+V5c9//jMzZszghRdeYPz48cybN4+xY8dywgknMHXqVAYNGsS0\nadM46KCDdjrWklRSEJGc16xZM4YNG8ZPfvKT7RqY161bx5577kleXh7Tp09naWk3ZM9wxBFH8Pjj\njwPwwQcfMHfuXCAMu73bbrvRokULVqxYwUsvvZR+TfPmzUuttx88eDDPPvssmzZt4ttvv+WZZ55h\n8ODBlf5sLVq0YI899kiXMv74xz8yZMgQtm3bxueff86wYcO4/fbbWbduHRs3buTjjz+mZ8+eXHvt\ntfzgBz/go48+qvR7lkclBRGpFc4880xGjBixXU+kUaNGcdJJJ9GzZ08KCgoq/MU8ZswYLrjgArp1\n60a3bt3SJY5DDjmEPn36cNBBB7HPPvtsN+z26NGjGT58OO3bt2f69Onp5X379uX888+nf//+AFx0\n0UX06dOn3Kqisjz66KNceumlbNq0ia5du/LII4+wdetWzj77bNatW4e7c8UVV9CyZUt++ctfMn36\ndBo0aECPHj3Sd5GrLho6W0TKpaGza5+qDJ2t6iMREUlTUhARkTQlBRGpUG2rZq7Pqvq3UlIQkXI1\nadKENWvWKDHUAu7OmjVrqnRBm3ofiUi5OnbsyLJly1i1alXSoUgWmjRpQseOHXf69UoKIlKuvLw8\nunTpknQYUkNUfSQiImlKCiIikqakICIiaUoKIiKSpqQgIiJpSgoiIpKmpCAiImlKCiIikqakICIi\naUoKIiKSpqQgIiJpsSUFM9vHzKab2YdmNt/MrixlGzOzCWa22MzmmlnfuOIREZGKxTkgXhHwH+4+\n28yaA7PM7BV3/zBjm+OA/aNpAPBA9CgiIgmIraTg7svdfXY0vwFYAHQosdkpwGMevA20NLO944pJ\nRETKVyNtCmaWD/QB3imxqgPwecbzZeyYODCz0WY208xmakx3EZH4xJ4UzKwZ8DRwlbuv35l9uPsk\ndy9w94K2bdtWb4AiIpIWa1IwszxCQih09/8tZZMvgH0ynneMlomISALi7H1kwO+BBe5+dxmbPQ+c\nG/VCGgisc/flccUkIiLli7P30SDgHGCemc2Jlo0DOgG4+4PAVOB4YDGwCbggxnhERKQCsSUFd38d\nsAq2ceCyuGIQEZHK0RXNIiKSpqQgIiJpSgoiIpKmpCAiImlKCiIikqakICIiaUoKIiKSpqQgIiJp\nSgoiIpKmpCAiImlKCiIikqakICIiaUoKIiKSpqQgIiJpSgoiIpJWb5LCli3wzDPgnnQkIiK5q94k\nhUcfhVNPhb/+NelIRERyV71JCuecA506wbhxKi2IiJSl3iSFxo3hpptg5kx49tmkoxERyU31JilA\nKC0ceCBcfz1s3Zp0NCIiuadeJYVGjeCWW+DDD+Hxx5OORkQk99SrpABw2mnQp0+oSvr++6SjERHJ\nLfUuKTRoAOPHwyefwMMPJx2NiEhuqXdJAWD4cDj8cLj5Zti8OeloRERyR71MCmbwq1/B8uUwcWLS\n0YiI5I56mRQABg8OJYZf/xrWr086GhGR3FBvkwLArbfC11/D3XcnHYmISG6o10mhXz8YORLuugtW\nr046GhGR5NXrpAChsXnTJrjttqQjERFJXr1PCt26wbnnwv33w7JlSUcjIpKsep8UAG68EbZtC20M\nIiL1WWxJwcweNrOVZvZBGeuHmtk6M5sTTTfEFUtF8vPhkkvg97+HxYuTikJEJHlxlhT+AAyvYJvX\n3L13NN0cYywVuu46yMsLw1+IiNRXsSUFd58BfB3X/qtbu3Zw5ZVhoLx585KORkQkGUm3KRxmZnPN\n7CUz65FwLPziF7D77vDLXyYdiYhIMpJMCrOBTu7eC7gPKPPWN2Y22sxmmtnMVatWxRZQq1YhMTz3\nHLzzTmxvIyKSsxJLCu6+3t03RvNTgTwza1PGtpPcvcDdC9q2bRtrXFdeCW3bhjaGlMLC0BjdoEF4\nLCyMNQQRkcQ0SuqNzawdsMLd3cz6ExLUmqTiSWnWLCSEq66Cv/0NvvoKRo8OF7gBLF0angOMGpVc\nnCIicTCP6S72ZvYEMBRoA6wAbgTyANz9QTO7HBgDFAGbgavd/c2K9ltQUOAzZ86MJeaU776DAw6A\n9u3DSKqffbbjNp07w5IlsYYhIlJtzGyWuxdUuF1cSSEuNZEUIFyzcNFFZa83Cxe8iYjUBtkmhaR7\nH+Ws886D/fcP1y6UplOnmo1HRKQmZJUUzGxfM2sczQ81syvMrGW8oSWrUSO45RbYsgV22WX7dU2b\nhlt6iojUNdmWFJ4GtprZfsAkYB/g8diiyhE/+hEccgi0bBlKBmahLWHSJDUyi0jdlG1S2ObuRcAI\n4D53/wWwd3xh5YYGDcIgeStXhh5J27aFxmUlBBGpq7JNClvM7EzgPODFaFkZte11ywknwKGHhvsu\nbN6cdDQiIvHKNilcABwKjHf3T82sC/DH+MLKHWbwq1/BF1/AAw8kHY2ISLwq3SXVzPYA9nH3ufGE\nVL6a6pJa0rHHwuzZ8MEHYfA8EZHapFq7pJrZ381sdzNrRRiz6CEzq1e3u7/jjlB9dNhh8K9/JR2N\niEg8sq0+auHu64FTgcfcfQBwdHxh5Z7evWH6dNiwAQYN0oB5IlI3ZZsUGpnZ3sDpFDc01zv9+8Nb\nb4XhtYcNgxfr7ZEQkboq26RwMzAN+Njd3zOzrsCi+MLKXfvtB2++CT16wA9/CL/7XdIRiYhUn6yS\ngrv/j7v3cvcx0fNP3P20eEPLXXvtFaqSjj4aLr44dFetZUNIiYiUKtuG5o5m9oyZrYymp82sY9zB\n5bJmzeCFF8IYSTfeCJdcAkVFSUclIlI12VYfPQI8D7SPpheiZfVaXh488giMGwcPPQSnnlp83wUR\nkdoo26TQ1t0fcfeiaPoDEO8t0GoJszA43sSJoeH5yCNh9eqkoxIR2TnZJoU1Zna2mTWMprPJgbuk\n5ZKf/hSefhrefz90Wf3006QjEhGpvGyTwk8I3VG/ApYDI4HzY4qp1hoxAv76V1i1Klzk9s9/Jh2R\niEjlZNv7aKm7n+zubd19T3f/IVBvex+VZ9AgeOONcA+GI46AV15JOiIRkexV5c5rV1dbFHVMt27h\nWoauXeH442Hy5KQjEhHJTlWSglVbFHVQhw4wYwYMHgznnBPGTtK1DCKS66qSFHSKq0CLFvDSS3DG\nGXDttXDFFbong4jktnKTgpltMLP1pUwbCNcrSAUaN4bCQrj6arj/fsjPD11Yv/km6chERHZUblJw\n9+buvnspU3N3b1RTQdZ2DRrAXXfB3/8O/frB9deHez5fc024eY+ISK6oSvWRVNKQITB1KsyZAyef\nDPfcA126wIUXwkcfJR2diIiSQiIOOSRUKS1eDKNHw+OPQ/fuYZgM3adBRJKkpFADCgtDW0KDBuGx\nsDAs79IltDMsXQrXXReqlwYOhKFD4eWX1VtJRGqekkLMCgtDaWDp0nCSX7o0PE8lBoA994Rbbgnr\n7rorlCCOOw769IEnntDoqyK1nTssXAiffQbbtiUdTfnMa9nP0YKCAp85c2bSYWQtPz+c7Evq3BmW\nLCn9Nd9/H6qUbr89tDV06RIapS+4AHbdNc5oRaS6fPllGPbmlVfC41dfheVNmsC++8L++4ebdu2/\nf/HUvn2oUYiDmc1y94IKt1NSiFeDBqVXA5lV/Ith27Zwz4bbboO334a2beHnP4fLLgu3BBWR3LFh\nA/zjH8WJ4MMPw/K2beGoo8K0bRssWlQ8ffwx/PvfxfvYddfihFEyabRvH84bO0tJIUfsTEmhJHd4\n7TX41a9g2jRo2TJcCHflldCqVXVGKyLZKiqC994LCeCVV8IPt6KiUBI44gg45phwd8Zevcr+9b9t\nGyxbtn2iWLy4OGF8/33xtk2bwtix8Mtf7ly8Sgo5ItWmkHnznaZNYdIkGDWq8vubOTNc/Pbss+Hu\nbz/9abgwbq+9qi9mEdmRO/zrX8XVQdOnw/r14dd7v34hARxzTBghuUmTqr/f1q3w+efbJ4ohQ+CU\nU3Zuf0oKOaSwMPQu+uyzcNHa+PE7lxAyffBBKDn86U9hRNaLL4Zf/AL22ad6YhapL7ZsCcPdr1gB\nK1eGx5LTypWhjWDVqvCaLl2KSwJHHgmtWyf7GbKReFIws4eBE4GV7n5wKesN+A1wPLAJON/dZ1e0\n39qYFOK0aFFoc3jssfCL5fzzQxGza9ekIxOpPtu2hU4Xa9eGk3hR0Y6PpS3LXPf997BmzY4n/jVl\n3C6sSZNQAs+cCgpCMqiN/1+5kBSOADYCj5WRFI4HfkZICgOA37j7gIr2q6RQuqVLw0isv/99+Ac4\n6yz4z/8Mw3iL1EbLl4eqmr/8JVTXrFhR9X02b779SX7PPXc88aemZs2q1rCbaxJPClEQ+cCLZSSF\n3wJ/d/cnoucLgaHuvry8fSoplG/58nCtwwMPhBFZTzstVF317p10ZCLl27QpdKhIJYJ588Lytm2L\nq2rat4e8PGjUaMfH0paVtk19lW1SSPIQdQA+z3i+LFq2Q1Iws9HAaIBOnTrVSHC11d57w513hiqk\ne++F++6Dp56CE08MJYeBA+PrBy1SGdu2wdy5xUngtddC98xddgn3Ibn9djj22PJ770j1qxV5090n\nAZMglBQSDqdWaNMGbr01XPQ2cWIYfG/QINhjj5AYBg6EQw+F/v3DfR9EakJmldArr4T6fYCDDw7X\n3xxzTOjO2bRpsnHWZ0kmhS+AzL4yHaNlUo1atgzVR1deGUoMb7wR+lOnxlYyC4PxpZLEwIGhHUK/\nzOo399DbZs6cML3/fujxlrpJVGZde2q+5GPJZd9/X3xtzp57hgRw7LHF1UKSG5JsUzgBuJzihuYJ\n7t6/on2qTaF6rFsXLrx5662QJN5+G77+OqzbfXcYMKA4SQwYoIvk6rItW0LPnvffL04Cc+Zs3ytn\n331DNU7z5uF56rRR1mNpy8zCeF7HHKMqoSQk3tBsZk8AQ4E2wArgRiAPwN0fjLqk3g8MJ3RJvcDd\nKzzbKynEwz10b00libfeCg19qaE4DjwwJIdevaBHjzB17FhzvTO+/jrckGjffVW1UBXr1hWf/FOP\nH3xQfOVskyahKqd37+KpZ08Nq1IXJJ4U4qKkUHM2bgyliVSSePfd7bsFNm8eqp5SSSI1v7PJYs2a\n4is3Sz6mbl/aqFH4tXnYYaGN5LDDoEOH6vm8dUVq6ISFC8MVuKnHjz7afsiVtm23P/n37g0HHFC/\ne+jUZUoKEovVq8NAX/Pnhyk1n2owhPCrsnv37RNGjx7h5P311zue8FPzmfetNgtXf2cOCrb33qH0\n8uabIUGl6rc7ddo+SfTqFf+JzT10oVy/PgyEtn59xdO334Zj07p16AhQ8jE1n5eXXQxr1+544l+4\nMBzP1LGB0N/+gANCaa9nz+IE0K5d3eqHL+VTUpAatXr19kkiNaWGBYDQ1TBzgK/STvypx65doXHj\nst9vy5ZQ/fHGGyFJvPFG8f2umzYNVV2pRDFwYOh1VZZNm0L8ZU2rVoXHNWtC9UvqJJ/NuPh5eaF3\n1+67w267hQSyenUohZUlM3FkJo0WLYpLAAsXbn9sGzYMQy8ceGCYUknggANCMtXJX5QU6pA4xk6q\nKatWFSeP27OgAAAMGklEQVSKTz4JvUxSJ/4uXapn4LCUzz8vThJvvhnqy7duDeu6dw9DFBQV7Xiy\nz/xVncms+ITctm2YT53gs5maNy87sf373yHJpGLJZn7jxnClbeYJP/XYtWtIuiJlUVKoI6p7lNX6\n5NtvQ5tIKlH885/h2KV+gaemtm13XNamTejO27Bh0p+iWFGR6vtl5ykp1BHVcT8GEZFsk4J6Cue4\nzz6r3HIRkapQUshxZQ31pCGgRCQOSgo5bvz4HS/Wato0LBcRqW5KCjlu1KjQqNy5c+gN07mzGplF\nJD7qy1ALjBqlJCAiNUMlBRERSVNSEBGRNCUFERFJU1IQEZE0JQUREUlTUhARkTQlhXqgsDCModSg\nQXgsLEw6IhHJVbpOoY4rOcrq0qXhOejaBxHZkUoKddx1120/7DaE59ddl0w8IpLblBTqOI2yKiKV\noaRQx2mUVRGpDCWFOk6jrIpIZSgp1HEaZVVEKkO9j+oBjbIqItlSSUFERNKUFEREJE1JQURE0pQU\nJCsaKkOkflBDs1RIQ2WI1B8qKUiFNFSGSP2hpCAV0lAZIvWHkoJUSENliNQfsSYFMxtuZgvNbLGZ\njS1l/VAzW2dmc6LphjjjkZ2joTJE6o/YGprNrCEwETgGWAa8Z2bPu/uHJTZ9zd1PjCsOqbpUY/J1\n14Uqo06dQkJQI7NI3RNn76P+wGJ3/wTAzKYApwAlk4LUAhoqQ6R+iLP6qAPwecbzZdGykg4zs7lm\n9pKZ9ShtR2Y22sxmmtnMVatWxRGriIiQfEPzbKCTu/cC7gOeLW0jd5/k7gXuXtC2bdsaDVCqhy5+\nE6kd4kwKXwD7ZDzvGC1Lc/f17r4xmp8K5JlZmxhjkgSkLn5buhTciy9+U2IQyT1xJoX3gP3NrIuZ\n7QKcATyfuYGZtTMzi+b7R/GsiTEmSYAufhOpPWJraHb3IjO7HJgGNAQedvf5ZnZptP5BYCQwxsyK\ngM3AGe7uccUkydDFbyK1R6xjH0VVQlNLLHswY/5+4P44Y5DkdeoUqoxKWy4iuSXphmapB3Txm0jt\noaQgsdN9okVqDyUFqRGjRsGSJbBtW3isbEJQl1aRmqH7KUjO0/0cRGqOSgqS89SlVaTmKClIzlOX\nVpGao6QgOU/3cxCpOUoKkvOqo0urGqpFsqOkIDmvql1aNfaSSPasto0qUVBQ4DNnzkw6DKlF8vNL\nv6K6c+fQPVakPjCzWe5eUNF2KilInaeGapHsKSlInVcdDdVqk5D6QklB6ryqNlSrTULqEyUFqfOq\n2lCti+ekPlFSkHqhKmMvVUebhKqfpLZQUhCpQFXbJFT9JLWJkoJIBaraJqHqJ6lNlBREKlDVNglV\nP0ltoqQgkoWqtEnkQvWTkopkS0lBJGZJVz8pqUhlKCmIxCzp6qdcSCpSeygpiNSAJKufkk4qUPWS\nhkoqNUdJQSTHVbX6KemkUtWSRi5Uf9WrpOTutWrq16+fi9Q3kye7d+7sbhYeJ0+u3GubNnUPp9Qw\nNW2a/T46d97+tampc+fa8fqqfv6qvj61j539+1XH693dgZmexTk28ZN8ZSclBZHKSzKpmJV+Ujer\nmdcrKQXZJgXdT0FEKlRYGNoQPvssVDuNH599u0hV72dR1dc3aBBOpSWZhTaeuF+f9OdP0f0URKTa\nVKWhvKptIkm3qSTdJlPT9wNRUhCRWFW1S25VX1/fk1KlZVPHlEuT2hREpLKSbOhVm0LM1KYgIrVN\nVdpkquP1kH2bgpKCiEg9oIZmERGptFiTgpkNN7OFZrbYzMaWst7MbEK0fq6Z9Y0zHhERKV9sScHM\nGgITgeOA7sCZZta9xGbHAftH02jggbjiERGRisVZUugPLHb3T9z9e2AKcEqJbU4BHosax98GWprZ\n3jHGJCIi5YgzKXQAPs94vixaVtltMLPRZjbTzGauWrWq2gMVEZGgUdIBZMPdJwGTAMxslZmVctF3\nTmgDrE46iHLkenyQ+zEqvqpRfFVTlfg6Z7NRnEnhC2CfjOcdo2WV3WY77t62WqKLgZnNzKbLV1Jy\nPT7I/RgVX9UovqqpifjirD56D9jfzLqY2S7AGcDzJbZ5Hjg36oU0EFjn7stjjElERMoRW0nB3YvM\n7HJgGtAQeNjd55vZpdH6B4GpwPHAYmATcEFc8YiISMVibVNw96mEE3/msgcz5h24LM4YatikpAOo\nQK7HB7kfo+KrGsVXNbHHV+uGuRARkfhomAsREUlTUhARkTQlhUoys33MbLqZfWhm883sylK2GWpm\n68xsTjTdUMMxLjGzedF77zCkbJJjTpnZgRnHZY6ZrTezq0psU+PHz8weNrOVZvZBxrJWZvaKmS2K\nHvco47XljvEVY3z/bWYfRX/DZ8ysZRmvLff7EGN8N5nZFxl/x+PLeG1Sx+9PGbEtMbM5Zbw21uNX\n1jklse9fNjdd0FQ8AXsDfaP55sC/gO4lthkKvJhgjEuANuWsPx54CTBgIPBOQnE2BL4COid9/IAj\ngL7ABxnL7gDGRvNjgdvL+AwfA12BXYD3S34fYozvWKBRNH97afFl832IMb6bgGuy+A4kcvxKrL8L\nuCGJ41fWOSWp759KCpXk7svdfXY0vwFYQClDc+S4XBlz6ijgY3dP/Ap1d58BfF1i8SnAo9H8o8AP\nS3lpNmN8xRKfu//F3Yuip28TLv5MRBnHLxuJHb8UMzPgdOCJ6n7fbJRzTknk+6ekUAVmlg/0Ad4p\nZfVhUbH+JTPrUaOBgQN/NbNZZja6lPVZjTlVA86g7H/EJI9fyl5efDHlV8BepWyTK8fyJ4TSX2kq\n+j7E6WfR3/HhMqo/cuH4DQZWuPuiMtbX2PErcU5J5PunpLCTzKwZ8DRwlbuvL7F6NtDJ3XsB9wHP\n1nB4h7t7b8LQ5JeZ2RE1/P4Viq5yPxn4n1JWJ338duChrJ6T/bfN7DqgCCgsY5Okvg8PEKo1egPL\nCVU0uehMyi8l1MjxK++cUpPfPyWFnWBmeYQ/XqG7/2/J9e6+3t03RvNTgTwza1NT8bn7F9HjSuAZ\nQhEzU6XHnIrBccBsd19RckXSxy/DilS1WvS4spRtEj2WZnY+cCIwKjpx7CCL70Ms3H2Fu291923A\nQ2W8b9LHrxFwKvCnsrapieNXxjklke+fkkIlRfWPvwcWuPvdZWzTLtoOM+tPOM5raii+3cyseWqe\n0Bj5QYnNcmHMqTJ/nSV5/Ep4Hjgvmj8PeK6UbbIZ4ysWZjYc+L/Aye6+qYxtsvk+xBVfZjvViDLe\nN7HjFzka+Mjdl5W2siaOXznnlGS+f3G1qNfVCTicUIybC8yJpuOBS4FLo20uB+YTegK8DRxWg/F1\njd73/SiG66LlmfEZ4a54HwPzgIIaPoa7EU7yLTKWJXr8CAlqObCFUC97IdAa+BuwCPgr0Cratj0w\nNeO1xxN6jHycOt41FN9iQn1y6nv4YMn4yvo+1FB8f4y+X3MJJ6q9c+n4Rcv/kPreZWxbo8evnHNK\nIt8/DXMhIiJpqj4SEZE0JQUREUlTUhARkTQlBRERSVNSEBGRNCUFkYiZbbXtR3CtthE7zSw/c4RO\nkVwV6+04RWqZzR6GMxCpt1RSEKlANJ7+HdGY+u+a2X7R8nwzezUa8O1vZtYpWr6XhfsbvB9Nh0W7\namhmD0Vj5v/FzHaNtr8iGkt/rplNSehjigBKCiKZdi1RffTjjHXr3L0ncD9wb7TsPuBRDwP3FQIT\nouUTgH+4+yGEMfznR8v3Bya6ew9gLXBatHws0Cfaz6VxfTiRbOiKZpGImW1092alLF8CHOnun0QD\nl33l7q3NbDVh6IYt0fLl7t7GzFYBHd393xn7yAdecff9o+fXAnnufquZvQxsJIwG+6xHgwGKJEEl\nBZHseBnzlfHvjPmtFLfpnUAYi6ov8F40cqdIIpQURLLz44zHt6L5NwmjUgKMAl6L5v8GjAEws4Zm\n1qKsnZpZA2Afd58OXAu0AHYorYjUFP0iESm2q21/8/aX3T3VLXUPM5tL+LV/ZrTsZ8AjZvYLYBVw\nQbT8SmCSmV1IKBGMIYzQWZqGwOQocRgwwd3XVtsnEqkktSmIVCBqUyhw99VJxyISN1UfiYhImkoK\nIiKSppKCiIikKSmIiEiakoKIiKQpKYiISJqSgoiIpP1/qIU7u1FUANkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126c34668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npk2ErJsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126fdd908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on \n",
    "the test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크가 8에포크 이후로는 과적합하는 것 같습니다. 새로운 네트워크를 처음부터 8에포크 까지만 학습시키고 시험 집합을 평가해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "7982/7982 [==============================] - 1s 148us/step - loss: 2.5398 - acc: 0.5226 - val_loss: 1.6733 - val_acc: 0.6570\n",
      "Epoch 2/8\n",
      "7982/7982 [==============================] - 1s 100us/step - loss: 1.3712 - acc: 0.7121 - val_loss: 1.2758 - val_acc: 0.7210\n",
      "Epoch 3/8\n",
      "7982/7982 [==============================] - 1s 101us/step - loss: 1.0136 - acc: 0.7781 - val_loss: 1.1303 - val_acc: 0.7530\n",
      "Epoch 4/8\n",
      "7982/7982 [==============================] - 1s 114us/step - loss: 0.7976 - acc: 0.8251 - val_loss: 1.0539 - val_acc: 0.7590\n",
      "Epoch 5/8\n",
      "7982/7982 [==============================] - 1s 103us/step - loss: 0.6393 - acc: 0.8624 - val_loss: 0.9754 - val_acc: 0.7920\n",
      "Epoch 6/8\n",
      "7982/7982 [==============================] - 1s 100us/step - loss: 0.5124 - acc: 0.8921 - val_loss: 0.9102 - val_acc: 0.8140\n",
      "Epoch 7/8\n",
      "7982/7982 [==============================] - 1s 102us/step - loss: 0.4124 - acc: 0.9137 - val_loss: 0.8932 - val_acc: 0.8210\n",
      "Epoch 8/8\n",
      "7982/7982 [==============================] - 1s 130us/step - loss: 0.3355 - acc: 0.9290 - val_loss: 0.8732 - val_acc: 0.8260\n",
      "2246/2246 [==============================] - 0s 192us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9845061221509137, 0.7836153161175423]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
    "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 모델은 78%의 정확도를 기록했습니다. 데이터 집합이 고루 분포된 이진 분류 문제의 경우, 둘 중 하나를 찍는 분류기의 성능은 적확도는 50% 일 것입니다. \n",
    "하지만 우리의 경우는 46 클래스를 분류하는 문제이므로, 아무리 잘 찍어도 19% 를 넘기긴 힘들 것입니다. 따라서 78% 라는 정확도는 아주 좋습니다. 적어도 그냥 찍는 모델에 비해서는 말이죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1834372217275156"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 6\n",
    "## Generating predictions on new data\n",
    "\n",
    "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic predictions for all of the test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 6\n",
    "## 새로운 데이터에 대한 예측 모델 생성하기 \n",
    "\n",
    "`predict` 라는 메소드가 46개 토픽에 대한 확률 분포를 반환함을 알 수 있습니다. 자 그럼 모든 시험 데이터에 대해서 토픽을 예측해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in `predictions` is a vector of length 46:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prediction` 변수의 각 요소는 길이가 46인 벡터입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients in this vector sum to 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 벡터의 각 요소를 더하면 1이 됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest entry is the predicted class, i.e. the class with the highest probability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 그 중 가장 값이 높은 클래스로 예측합니다. 즉 가장 확률이 높은 클래스를 선택하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 7\n",
    "## A different way to handle the labels and the loss\n",
    "\n",
    "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 7\n",
    "## 레이블과 손실을 다루는 다른 방법\n",
    "\n",
    "앞서 레이블을 인코딩 하는 방식 중 intefer tensor로 인코딩하는 법에 대해서 말씀드렸습니다. 다음과 같은 방식입니다 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to \n",
    "follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 위해서는 손실 함수를 조금 손봐줘야 합니다. 앞서 레이블이 범주형 인코딩이었기 때문에 `categorical_crossentropy`를 사용하였습니다. integer 레이블들의 경우에는 `sparse_categorical_crossentropy`를 사용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 새로운 손실 함수는 수학적으로 `categorical_crossentropy`와 동일합니다. 다만 생긴 모습만 조금 다를 뿐입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 8\n",
    "## On the importance of having sufficiently large intermediate layers\n",
    "\n",
    "\n",
    "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
    "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
    "46-dimensional, e.g. 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 0s - loss: 3.1620 - acc: 0.2295 - val_loss: 2.6750 - val_acc: 0.2740\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 0s - loss: 2.2009 - acc: 0.3829 - val_loss: 1.7626 - val_acc: 0.5990\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 0s - loss: 1.4490 - acc: 0.6486 - val_loss: 1.4738 - val_acc: 0.6390\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 0s - loss: 1.2258 - acc: 0.6776 - val_loss: 1.3961 - val_acc: 0.6570\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 0s - loss: 1.0886 - acc: 0.7032 - val_loss: 1.3727 - val_acc: 0.6700\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.9817 - acc: 0.7494 - val_loss: 1.3682 - val_acc: 0.6800\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.8937 - acc: 0.7757 - val_loss: 1.3587 - val_acc: 0.6810\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.8213 - acc: 0.7942 - val_loss: 1.3548 - val_acc: 0.6960\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.7595 - acc: 0.8088 - val_loss: 1.3883 - val_acc: 0.7050\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.7072 - acc: 0.8193 - val_loss: 1.4216 - val_acc: 0.7020\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.6642 - acc: 0.8254 - val_loss: 1.4405 - val_acc: 0.7020\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.6275 - acc: 0.8281 - val_loss: 1.4938 - val_acc: 0.7080\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.5915 - acc: 0.8353 - val_loss: 1.5301 - val_acc: 0.7110\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.5637 - acc: 0.8419 - val_loss: 1.5400 - val_acc: 0.7080\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.5389 - acc: 0.8523 - val_loss: 1.5826 - val_acc: 0.7090\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.5162 - acc: 0.8588 - val_loss: 1.6391 - val_acc: 0.7080\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.4950 - acc: 0.8623 - val_loss: 1.6469 - val_acc: 0.7060\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.4771 - acc: 0.8670 - val_loss: 1.7258 - val_acc: 0.6950\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.4562 - acc: 0.8718 - val_loss: 1.7667 - val_acc: 0.6930\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 0s - loss: 0.4428 - acc: 0.8742 - val_loss: 1.7785 - val_acc: 0.7060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8ce7cdb9b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
    "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
    "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
    "of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph 9\n",
    "## Further experiments\n",
    "\n",
    "* Try using larger or smaller layers: 32 units, 128 units...\n",
    "* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
    "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
    "probability distribution over the N output classes.\n",
    "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
    "probability distributions output by the network, and the true distribution of the targets.\n",
    "* There are two ways to handle labels in multi-class classification:\n",
    "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
    "function.\n",
    "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
    "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
    "intermediate layers that are too small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
